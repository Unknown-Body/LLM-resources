# LLM-resources
all related resources about LLM，pretraining，finetuning，and data preparation

## practise
- [LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide)
- [personalized AI Tutor](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor)
- [building RAG-based LLM applications for production](https://github.com/ray-project/llm-applications)

## GPT related
- [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)
- [gpt4free](https://github.com/xtekky/gpt4free)
- [AgentGPT](https://github.com/reworkd/AgentGPT)
- [Text-Prompted Generative Audio Model](https://github.com/suno-ai/bark)
- [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)
- [chatgpt-retrieval-plugin](https://github.com/openai/chatgpt-retrieval-plugin)
- [gpt4-pdf-chatbot-langchain](https://github.com/mayooear/gpt4-pdf-chatbot-langchain)
- [MOSS](https://github.com/OpenLMLab/MOSS)
- [dolly](https://github.com/databrickslabs/dolly)
- [pdfGPT](https://github.com/bhaskatripathi/pdfGPT)
- [Auto-GPT-ZH](https://github.com/kaqijiang/Auto-GPT-ZH)
- [open-chat-video-editor](https://github.com/SCUTlihaoyu/open-chat-video-editor)

## Related GitHub
- [LMOps](https://github.com/microsoft/LMOps)

## Survey
- [A Survey of Large Language Models](https://github.com/RUCAIBox/LLMSurvey)
- [LLMs evaluation](https://github.com/MLGroupJLU/LLM-eval-survey)
- [llm-hallucination-survey](https://github.com/HillZhang1999/llm-hallucination-survey)
- [LLM-Agent-Paper-List](https://github.com/WooooDyy/LLM-Agent-Paper-List)
- [LLM-Agent-Survey](https://github.com/Paitesanshi/LLM-Agent-Survey)

## LLM related application papers 
- [A Survey on Large Language Models for Recommendation](https://arxiv.org/pdf/2305.19860v2.pdf)
- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://dl.acm.org/doi/pdf/10.1145/3560815)


## Prompt Related
- [promptsource](https://github.com/bigscience-workshop/promptsource)

## Open Awesome Resources
- [Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)
- [Awesome Pretrained Chinese NLP Models](https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models)
- [Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference?tab=readme-ov-file)


## LLM and KG
- [KG-LLM-Papers](https://github.com/zjukg/KG-LLM-Papers)
- [Awesome-LLM-KG](https://github.com/RManLuo/Awesome-LLM-KG)

## SFT
- [LLM-SFT](https://github.com/yongzhuo/LLM-SFT)
- [Fine-tuning ChatGLM-6B with PEFT](https://github.com/hiyouga/ChatGLM-Efficient-Tuning)
- [Llama2-SFT](https://github.com/yongzhuo/Llama2-SFT)
- [InternLM-SFT](https://github.com/yongzhuo/InternLM-SFT)
- [Qwen-SFT](https://github.com/yongzhuo/Qwen-SFT)

## Model Editing
- [Knowledge Editing for LLMs Papers](https://github.com/zjunlp/KnowledgeEditingPapers)
- [ConCoRD: Consistency Correction through Relation Detection](https://github.com/eric-mitchell/concord)
- [Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model](https://github.com/eric-mitchell/serac)

## NL2Code
- [NL2Code](https://nl2code.github.io/)

## LLM for finance
- [FinRobot](https://github.com/AI4Finance-Foundation/FinRobot)

## LLM catastrophic Forgetting
- [An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning](https://arxiv.org/pdf/2308.08747)
- [LEARNING TO LEARN WITHOUT FORGETTING BY MAXIMIZING TRANSFER AND MINIMIZING INTERFERENCE](https://arxiv.org/pdf/1810.11910)
- [Investigating Forgetting in Pre-Trained Representations Through Continual Learning](https://arxiv.org/pdf/2305.05968)
- [PRETRAINED LANGUAGE MODEL IN CONTINUAL LEARNING: A COMPARATIVE STUDY](https://openreview.net/pdf?id=figzpGMrdD)
- [Probing Representation Forgetting in Supervised and Unsupervised Continual Learning](https://arxiv.org/pdf/2203.13381)
- [Mitigating Catastrophic Forgetting in Task-Incremental Continual Learning with Adaptive Classification Criterion](https://arxiv.org/pdf/2305.12270)
- [AdaPrompt: Adaptive Model Training for Prompt-based NLP](https://arxiv.org/pdf/2202.04824)
- [Continual Training of Language Models for Few-Shot Learning](https://arxiv.org/pdf/2210.05549)
- [Dark Experience for General Continual Learning: a Strong, Simple Baseline](https://arxiv.org/pdf/2004.07211)
- 
- 
## LLM related data
- [MNBVC(Massive Never-ending BT Vast Chinese corpus)超大规模中文语料集](https://github.com/esbatmop/MNBVC)
- [中文医疗问答数据集](https://github.com/Toyhom/Chinese-medical-dialogue-data)

## Agent Related
- [Awesome-Foundation-Agents](https://github.com/FoundationAgents/awesome-foundation-agents)


## RL Related
- [Open Reasoner Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)
- [Awesome-RL-Reasoning-Recipes](https://github.com/TsinghuaC3I/Awesome-RL-Reasoning-Recipes)
